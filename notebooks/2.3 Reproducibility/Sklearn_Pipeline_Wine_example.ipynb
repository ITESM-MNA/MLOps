{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c6aa50bb",
      "metadata": {
        "id": "c6aa50bb"
      },
      "source": [
        "\n",
        "# FTI Design Pattern Pipelines with scikit-learn.\n",
        "## Example with the Wine Dataset\n",
        "\n",
        "This notebook implements the **FTI design pattern** — **Feature**, **Training**, and **Inference** pipelines — using scikit-learn on the classic **Wine** classification dataset.\n",
        "\n",
        "- **Feature Pipeline (F)**: builds a reproducible feature table from raw data (e.g., imputing, scaling, encoding), and persists artifacts (features, schemas, transformers).\n",
        "- **Training Pipeline (T)**: consumes the features/labels (or composes the feature pipeline internally), trains a model with cross-validation, and persists the **trained model** and metrics.\n",
        "- **Inference Pipeline (I)**: loads the deployed model and applies the **same feature logic** to new/unseen data to produce predictions, enabling batch or online inference.\n",
        "  \n",
        " FTI reference: Hopsworks' perspective of **Feature / Training / Inference pipelines** for well-structured ML systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae25b568",
      "metadata": {
        "id": "ae25b568"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd23f6f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd23f6f5",
        "outputId": "b4fb6bde-d2bd-411b-f2ca-07e900fe2dec"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5f15725",
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_repo_root(start: Path, marker_dir: str = \"data\") -> Path:\n",
        "\n",
        "    current = start.resolve()\n",
        "    for parent in [current] + list(current.parents):\n",
        "        if (parent / marker_dir).is_dir():\n",
        "            return parent\n",
        "    return current\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "REPO_ROOT = find_repo_root(Path.cwd(), marker_dir=\"data\")\n",
        "\n",
        "DATA_DIR = REPO_ROOT / 'data'\n",
        "ARTIFACTS_DIR = DATA_DIR / 'processed' / 'artifacts'\n",
        "FEATURES_DIR = ARTIFACTS_DIR / 'features'\n",
        "MODELS_DIR = ARTIFACTS_DIR / 'models'\n",
        "REPORTS_DIR = ARTIFACTS_DIR / 'reports'\n",
        "\n",
        "for d in [ARTIFACTS_DIR, FEATURES_DIR, MODELS_DIR, REPORTS_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1beeabde",
      "metadata": {
        "id": "1beeabde"
      },
      "source": [
        "## Load the Wine dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71c7d39b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "71c7d39b",
        "outputId": "f5762dc9-4cff-4962-8953-299d137edd29"
      },
      "outputs": [],
      "source": [
        "\n",
        "wine = load_wine(as_frame=True)\n",
        "X = wine.data.copy()\n",
        "y = wine.target.copy()\n",
        "feature_names = X.columns.tolist()\n",
        "target_name = 'target'\n",
        "\n",
        "print('Features:', feature_names[:5], '...')\n",
        "print('Target classes:', wine.target_names)\n",
        "\n",
        "# Persist a raw snapshot for provenance\n",
        "raw_df = X.copy()\n",
        "raw_df[target_name] = y\n",
        "raw_path = FEATURES_DIR / 'wine_raw_snapshot.csv'\n",
        "raw_df.to_csv(raw_path, index=False)\n",
        "print('Raw snapshot saved to:', raw_path)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "X_train.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "280b339e",
      "metadata": {
        "id": "280b339e"
      },
      "source": [
        "\n",
        "##  Feature Pipeline (F)\n",
        "\n",
        "**Goal:** Transform raw inputs into a clean, consistent **feature table** suitable for both training and inference.\n",
        "- Handles data quality concerns (e.g., missing values) and applies **scaling/encoding**.\n",
        "- Persists the **feature transformer** and **feature table** to ensure **training/serving parity**.\n",
        "- Produces: `features_train.npy`, `features_test.npy`, `feature_names.json`, and `feature_transformer.joblib`.\n",
        "\n",
        "**In this dataset:** All features are numeric, so we will apply `SimpleImputer(strategy='median')` followed by `StandardScaler`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "761f75cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "761f75cf",
        "outputId": "22f9b002-58ac-416d-ccf7-da74a6497bc3"
      },
      "outputs": [],
      "source": [
        "\n",
        "numeric_features = feature_names  # all numeric for Wine\n",
        "numeric_transform = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "feature_pipeline = ColumnTransformer(\n",
        "    transformers=[('num', numeric_transform, numeric_features)],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# Fit on training data only\n",
        "X_train_features = feature_pipeline.fit_transform(X_train)\n",
        "X_test_features  = feature_pipeline.transform(X_test)\n",
        "\n",
        "# Persist feature artifacts\n",
        "np.save(FEATURES_DIR / 'features_train.npy', X_train_features)\n",
        "np.save(FEATURES_DIR / 'features_test.npy',  X_test_features)\n",
        "\n",
        "with open(FEATURES_DIR / 'feature_names.json', 'w') as f:\n",
        "    json.dump(numeric_features, f, indent=2)\n",
        "\n",
        "joblib.dump(feature_pipeline, FEATURES_DIR / 'feature_transformer.joblib')\n",
        "\n",
        "print('Feature shapes:', X_train_features.shape, X_test_features.shape)\n",
        "print('Saved feature artifacts to:', FEATURES_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9604bad",
      "metadata": {
        "id": "b9604bad"
      },
      "source": [
        "\n",
        "##  Training Pipeline (T)\n",
        "\n",
        "**Goal:** Train and validate a model using the **feature pipeline**, and persist the **trained model** and **metrics**.\n",
        "\n",
        "Two common approaches:\n",
        "1. **Compose** the feature pipeline inside the training pipeline (ensures end-to-end reproducibility from raw data).\n",
        "2. **Consume** pre-computed features from the Feature pipeline (useful in production/batch scenarios).\n",
        "\n",
        "Here, we demonstrate **Approach 1** by creating a single scikit-learn `Pipeline` that nests the feature transformer and classifier.  \n",
        "This guarantees the same preprocessing at inference time if we export the **fitted pipeline** as a single artifact.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "417df19c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "417df19c",
        "outputId": "d2de8e71-f6a4-46da-9923-23827694c043"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timezone\n",
        "\n",
        "# Define the full training pipeline: Feature engineering + Model\n",
        "full_training_pipeline = Pipeline(steps=[\n",
        "    ('features', feature_pipeline),\n",
        "    ('clf', LogisticRegression(max_iter=200, random_state=RANDOM_STATE))\n",
        "])\n",
        "\n",
        "# Hyperparameter grid for demonstration\n",
        "param_grid = {\n",
        "    'clf__C': [0.1, 1.0, 10.0],\n",
        "    'clf__penalty': ['l2'],\n",
        "    'clf__solver': ['lbfgs']  # good default for multinomial problems\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=full_training_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "best_model = grid.best_estimator_\n",
        "print('Best params:', grid.best_params_)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print('Test accuracy:', acc)\n",
        "\n",
        "# Persist model & a simple report\n",
        "model_path = MODELS_DIR / 'wine_fti_model.joblib'\n",
        "joblib.dump(best_model, model_path)\n",
        "\n",
        "\n",
        "report = {\n",
        "    'timestamp': datetime.now(timezone.utc).isoformat() + 'Z',\n",
        "    'best_params': grid.best_params_,\n",
        "    'test_accuracy': float(acc),\n",
        "    'classification_report': classification_report(y_test, y_pred, output_dict=True)\n",
        "}\n",
        "with open(REPORTS_DIR / 'evaluation_report.json', 'w') as f:\n",
        "    json.dump(report, f, indent=2)\n",
        "\n",
        "print('Model saved to:', model_path)\n",
        "print('Report saved to:', REPORTS_DIR / 'evaluation_report.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "209f078b",
      "metadata": {
        "id": "209f078b"
      },
      "source": [
        "### (Optional) Quick Confusion Matrix Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3df2c0a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "3df2c0a3",
        "outputId": "a7a3aef6-acd8-48b4-a3f3-c7758c770ff5"
      },
      "outputs": [],
      "source": [
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "fig, ax = plt.subplots(figsize=(4, 4))\n",
        "im = ax.imshow(cm)\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.set_xlabel('Predicted')\n",
        "ax.set_ylabel('True')\n",
        "for (i, j), v in np.ndenumerate(cm):\n",
        "    ax.text(j, i, str(v), ha='center', va='center')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbf0ac6c",
      "metadata": {
        "id": "cbf0ac6c"
      },
      "source": [
        "\n",
        "##  Inference Pipeline (I)\n",
        "\n",
        "**Goal:** Load the **deployed model** artifact and apply it to **new/unseen data** to produce predictions.\n",
        "\n",
        "Key requirements:\n",
        "- **Same feature logic** as training (we satisfy this by exporting the *entire* fitted pipeline).\n",
        "- Support **batch** (this notebook) or **online** (e.g., FastAPI) inference.\n",
        "- Persist predictions if needed.\n",
        "\n",
        "Below we simulate a batch inference job using a slice of the test set as \"new data\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49c882ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "49c882ea",
        "outputId": "14351356-30c5-4216-e48d-9214461d651e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the deployed/fitted pipeline\n",
        "deployed_pipeline = joblib.load(MODELS_DIR / 'wine_fti_model.joblib')\n",
        "\n",
        "# Simulate \"new\" data: take 5 rows from the test set\n",
        "new_data = X_test.sample(5, random_state=RANDOM_STATE).copy()\n",
        "new_true = y_test.loc[new_data.index]\n",
        "\n",
        "preds = deployed_pipeline.predict(new_data)\n",
        "probs = deployed_pipeline.predict_proba(new_data)\n",
        "\n",
        "inference_df = new_data.copy()\n",
        "inference_df['predicted_class'] = preds\n",
        "inference_df['true_class'] = new_true.values\n",
        "\n",
        "# Persist batch predictions\n",
        "pred_path = REPORTS_DIR / 'batch_predictions.csv'\n",
        "inference_df.to_csv(pred_path, index=False)\n",
        "\n",
        "print('Batch predictions saved to:', pred_path)\n",
        "inference_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f09e9130",
      "metadata": {
        "id": "f09e9130"
      },
      "source": [
        "\n",
        "##  How the pipelines are **connected**\n",
        "\n",
        "- **Features**: The **Feature pipeline** (`feature_pipeline`) defines the transformations for the input table.  \n",
        "  In training, we **compose** it into the `full_training_pipeline` to ensure parity. In inference, we **load the fitted full pipeline** so the exact same transformations are applied.\n",
        "\n",
        "- **Labels**: The target (`wine.target`) is kept separate from feature engineering, then used in the **Training pipeline** for fitting and evaluation.\n",
        "\n",
        "- **Models**: The **Training pipeline** produces a single serialized artifact (`wine_fti_model.joblib`) that encapsulates **both preprocessing and the classifier**.  \n",
        "  The **Inference pipeline** loads this artifact and runs `predict` / `predict_proba` on new data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a125e50e",
      "metadata": {
        "id": "a125e50e"
      },
      "source": [
        "\n",
        "##  Artifacts produced\n",
        "\n",
        "- `artifacts/features/wine_raw_snapshot.csv` – raw snapshot of features + label\n",
        "- `artifacts/features/features_train.npy`, `features_test.npy` – example persisted feature matrices\n",
        "- `artifacts/features/feature_names.json` – schema of features\n",
        "- `artifacts/features/feature_transformer.joblib` – feature transformer (if needed separately)\n",
        "- `artifacts/models/wine_fti_model.joblib` – **deployed** (fitted) pipeline (features + model)\n",
        "- `artifacts/reports/evaluation_report.json` – metrics and best params\n",
        "- `artifacts/reports/batch_predictions.csv` – example batch inference output\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
