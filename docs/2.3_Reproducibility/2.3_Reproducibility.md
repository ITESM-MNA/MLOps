# Ensuring Reproducibility with ML Models Using MLflow and the Wine Dataset

## Table of Contents

- [Introduction](#introduction)
- [Objectives](#objectives)
- [Prerequisites](#prerequisites)
- [Dataset Description](#dataset-description)
- [Setup Instructions](#setup-instructions)
- [Activity Steps](#activity-steps)
  - [1. Environment Setup](#1-environment-setup)
  - [2. Load and Explore the Dataset](#2-load-and-explore-the-dataset)
  - [3. Preprocess the Data](#3-preprocess-the-data)
  - [4. Initialize MLflow Tracking](#4-initialize-mlflow-tracking)
  - [5. Define Training and Logging Function](#5-define-training-and-logging-function)
  - [6. Run Experiments with Different Models](#6-run-experiments-with-different-models)
    - [a. Logistic Regression](#a-logistic-regression)
    - [b. Decision Tree Classifier](#b-decision-tree-classifier)
    - [c. Random Forest Classifier](#c-random-forest-classifier)
  - [7. Compare Experiment Results](#7-compare-experiment-results)
  - [8. Reproduce an Experiment](#8-reproduce-an-experiment)
  - [9. Document Your Findings](#9-document-your-findings)
- [Expected Outcomes](#expected-outcomes)
- [Additional Challenges (Optional)](#additional-challenges-optional)
- [Tips](#tips)
- [Conclusion](#conclusion)

---

## Introduction

This activity aims to emphasize the importance of reproducibility in machine learning workflows. You will use **MLflow**, an open-source platform for managing the ML lifecycle, to track and manage your experiments. By working with the classic **Wine Dataset**, you will build, evaluate, and compare classification models. This hands-on experience will help you understand how to use MLflow to ensure that your machine learning experiments are reproducible and easily shareable.

The following task is performed during model selection and validation phase. It happens before deployment, so it priors refactoring.

## Objectives

- **Understand** the concept and significance of reproducibility in machine learning.
- **Learn** how to use MLflow for tracking experiments, parameters, metrics, and models.
- **Implement** multiple machine learning models using the Wine Dataset.
- **Ensure** reproducibility by recreating results using MLflow.

## Prerequisites

- Basic understanding of Python programming and machine learning concepts.
- Familiarity with scikit-learn for model building.
- Installed packages:
  - `mlflow`
  - `scikit-learn`
  - `pandas`
  - `numpy`
  - `matplotlib` (optional, for visualization)

## Dataset Description

The Wine Dataset is a classic dataset available in scikit-learn. It consists of 178 instances of wines classified into three categories based on 13 chemical attributes.

```python
from sklearn.datasets import load_wine
data = load_wine()
```

Features include alcohol content, malic acid, ash, alkalinity, magnesium, and more.

## Setup Instructions


1. **Create a Virtual Environment** (Optional but recommended, instead of virtual envs for Anaconda):

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows use `venv\Scripts\activate`
   ```

2. **Install Required Packages**:

   ```bash
   pip install mlflow scikit-learn pandas numpy matplotlib
   ```

## Activity Steps

### 1. Environment Setup

- **Install MLflow** (if not already installed):

  ```bash
  pip install mlflow
  ```

- **Import Necessary Libraries**:

  ```python
  import mlflow
  import mlflow.sklearn
  import pandas as pd
  import numpy as np
  from sklearn.datasets import load_wine
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  from sklearn.metrics import accuracy_score, precision_score, recall_score
  from sklearn.linear_model import LogisticRegression
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.ensemble import RandomForestClassifier
  ```

### 2. Load and Explore the Dataset

- **Load the Data**:

  ```python
  data = load_wine()
  X = pd.DataFrame(data.data, columns=data.feature_names)
  y = pd.Series(data.target)
  ```

- **Explore the Data** (Optional):

  ```python
  print(X.head())
  print(y.value_counts())
  print(X.isnull().sum())
  ```

  - Check for missing values.
  - Visualize feature distributions using histograms or box plots.
  - Analyze feature correlations using a heatmap.

### 3. Preprocess the Data

- **Split the Data**:

  ```python
  X_train, X_test, y_train, y_test = train_test_split(
      X, y, test_size=0.2, random_state=42, stratify=y
  )
  ```

- **Scale the Features** (Recommended):

  ```python
  scaler = StandardScaler()
  X_train_scaled = scaler.fit_transform(X_train)
  X_test_scaled = scaler.transform(X_test)
  ```

### 4. Initialize MLflow Tracking

- **Set the Experiment**:

  ```python
  mlflow.set_experiment("Wine_Classification")
  ```

### 5. Define Training and Logging Function

The *hyperparamateres* are key component for reproducibility in ML, for this reason these values should be stored during training for each model.

```python
def train_and_log_model(model, model_name, X_train, X_test, y_train, y_test, params):
    with mlflow.start_run(run_name=model_name):
        # Train the model
        model.fit(X_train, y_train)
        # Make predictions
        y_pred = model.predict(X_test)
        # Calculate metrics
        acc = accuracy_score(y_test, y_pred)
        prec = precision_score(y_test, y_pred, average='weighted')
        rec = recall_score(y_test, y_pred, average='weighted')
        # Log parameters and metrics
        mlflow.log_params(params)
        mlflow.log_metrics({"accuracy": acc, "precision": prec, "recall": rec})
        # Log the model
        mlflow.sklearn.log_model(model, artifact_path="models")
```

### 6. Run Experiments with Different Models

#### a. Logistic Regression

- **Set Parameters**:

  ```python
  params_lr = {"C": 1.0, "solver": "liblinear", "random_state": 42}
  model_lr = LogisticRegression(**params_lr)
  ```

- **Train and Log**:

  ```python
  train_and_log_model(
      model=model_lr,
      model_name="Logistic_Regression",
      X_train=X_train_scaled,
      X_test=X_test_scaled,
      y_train=y_train,
      y_test=y_test,
      params=params_lr
  )
  ```

#### b. Decision Tree Classifier

- **Set Parameters**:

  ```python
  params_dt = {"max_depth": 5, "criterion": "entropy", "random_state": 42}
  model_dt = DecisionTreeClassifier(**params_dt)
  ```

- **Train and Log**:

  ```python
  train_and_log_model(
      model=model_dt,
      model_name="Decision_Tree",
      X_train=X_train,
      X_test=X_test,
      y_train=y_train,
      y_test=y_test,
      params=params_dt
  )
  ```

#### c. Random Forest Classifier

- **Set Parameters**:

  ```python
  params_rf = {"n_estimators": 100, "max_depth": 5, "random_state": 42}
  model_rf = RandomForestClassifier(**params_rf)
  ```

- **Train and Log**:

  ```python
  train_and_log_model(
      model=model_rf,
      model_name="Random_Forest",
      X_train=X_train,
      X_test=X_test,
      y_train=y_train,
      y_test=y_test,
      params=params_rf
  )
  ```

### 7. Compare Experiment Results

- **Launch MLflow UI**:

  In your terminal, run:

  ```bash
  mlflow ui
  ```

- **Access the UI**:

  Open your web browser and navigate to `http://localhost:5000`.

- **Compare Runs**:

  - Select the "Wine_Classification" experiment.
  - Compare the metrics and parameters of different runs.
  - Use the sorting and comparison features to analyze performance.
  - Identify the best-performing model based on accuracy, precision, and recall.

### 8. Reproduce an Experiment

- **Select a Run**:

  Note the `run_id` of the best-performing model from the MLflow UI.

- **Load the Model**:

  ```python
  run_id = "<your-selected-run-id>"
  logged_model = f'runs:/{run_id}/models'

  # Load model as a sklearn model.
  loaded_model = mlflow.sklearn.load_model(logged_model)
  ```

- **Re-evaluate the Model**:

  ```python
  y_pred_loaded = loaded_model.predict(X_test_scaled)
  acc_loaded = accuracy_score(y_test, y_pred_loaded)
  print(f"Reproduced Accuracy: {acc_loaded}")
  ```

- **Verify Reproducibility**:

  Ensure that the reproduced accuracy matches the logged accuracy in MLflow.

### 9. Document Your Findings

- **Create a Report or Notebook**:

  - Summarize the performance of different models.
  - Discuss the importance of reproducibility in machine learning.
  - Explain how MLflow facilitated experiment tracking and reproducibility.
  - Include code snippets and, if possible, screenshots from the MLflow UI.
  - Reflect on any challenges faced and how you overcame them.

## Expected Outcomes

- A set of MLflow experiment runs tracking different models, parameters, and metrics.
- Ability to reproduce any experiment using the logged information in MLflow.
- A comparative analysis of model performances.
- A documented report highlighting the importance of reproducibility and how MLflow assists in achieving it.

## Additional Challenges (Optional)

- **Hyperparameter Tuning**:

  Use scikit-learn's `GridSearchCV` or `RandomizedSearchCV` to perform hyperparameter tuning. Log each hyperparameter combination as a separate MLflow run.

- **Implement Additional Models**:

  Try other algorithms like Support Vector Machines (SVM) or Gradient Boosting Machines.

- **Data Versioning**:

  Integrate version control on your datasets within MLflow or integrate MLflow with DVC (Data Version Control) for comprehensive data and experiment tracking.

## Tips

- **Consistent Random States**:

  Use `random_state=42` (or any fixed number) in your models and data splitting to ensure consistency across runs.

- **Logging Custom Metrics**:

  If you compute additional metrics (e.g., F1-score, ROC-AUC), make sure to log them in MLflow using `mlflow.log_metric()`.

- **Artifact Management**:

  You can log artifacts like plots, the scaler object, or confusion matrices using `mlflow.log_artifact()`.

  ```python
  # Example of logging an artifact
  import matplotlib.pyplot as plt
  from sklearn.metrics import plot_confusion_matrix

  fig, ax = plt.subplots()
  plot_confusion_matrix(model, X_test_scaled, y_test, ax=ax)
  plt.savefig("confusion_matrix.png")
  mlflow.log_artifact("confusion_matrix.png")
  ```
